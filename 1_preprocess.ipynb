{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import spacy\n",
    "import re\n",
    "from typing import List, Dict, Tuple, Set, Union\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import Model\n",
    "import json\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "def tokenize(text:str)->List[str]:\n",
    "    \"\"\"\n",
    "    Tokenize text using spacy.\n",
    "    \"\"\"\n",
    "    text = text.lower()\n",
    "    text = re.sub(r\"[^a-zA-Z0-9]\", \" \", text)\n",
    "    #remove numbers and punctuation\n",
    "    text = re.sub(r\"[0-9]\", \"\", text)\n",
    "    doc = nlp(text)\n",
    "    return [token.text for token in doc]\n",
    "\n",
    "def create_tokenmap(tokens:List[str])->Dict[str, int]:\n",
    "    \"\"\"\n",
    "    Create a token map for the given tokens.\n",
    "    \"\"\"\n",
    "    tokenmap = {}\n",
    "    for token in tokens:\n",
    "        if token not in tokenmap:\n",
    "            tokenmap[token] = 1\n",
    "        else:\n",
    "            tokenmap[token] += 1\n",
    "    return tokenmap\n",
    "\n",
    "def assemble_token_maps(\n",
    "    token_maps:List[Dict[str,int]]\n",
    "    )->Dict[Dict[Union[str,int],Union(str,int)]]:\n",
    "    \"\"\"\n",
    "    Assemble all token maps into one.\n",
    "    \"\"\"\n",
    "    tokenmap = {}\n",
    "    for token_map in token_maps:\n",
    "        for token in token_map:\n",
    "            if token not in tokenmap:\n",
    "                tokenmap[token] = token_map[token]\n",
    "            else:\n",
    "                tokenmap[token] += token_map[token]\n",
    "    \n",
    "    tok_to_idx = {\"<PAD>\": 0, \"<UNK>\": 1}\n",
    "    idx_to_tok = {0: \"<PAD>\", 1: \"<UNK>\"}\n",
    "    idx_to_cnt = {0: 0, 1: 0}\n",
    "    for idx, token in enumerate(tokenmap):\n",
    "        tok_to_idx[token] = idx\n",
    "        idx_to_tok[idx] = token\n",
    "        idx_to_cnt[idx] = tokenmap[token]\n",
    "\n",
    "    tokenmap = {\n",
    "        \"tok_to_idx\": tok_to_idx,\n",
    "        \"idx_to_tok\": idx_to_tok,\n",
    "        \"idx_to_cnt\": idx_to_cnt\n",
    "    }\n",
    "    return tokenmap\n",
    "    \n",
    "def preprocess_text(\n",
    "    text:str,tokenmap:Dict[Dict[Union[str,int],Union(str,int)]]\n",
    "    )->List[int]:\n",
    "    \"\"\"\n",
    "    tokenize text and replace words with indices.\n",
    "    \"\"\"\n",
    "    text = [tokenmap[\"tok_to_idx\"].get(token, 1) for token in text]\n",
    "    return text\n",
    "\n",
    "\n",
    "def test_train_split(\n",
    "    df:pd.DataFrame, test_size:float=0.2, random_state:int=42\n",
    "    )->Tuple[pd.DataFrame, pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Split data into train and test set.\n",
    "    \"\"\"\n",
    "    df = df.sample(frac=1, random_state=random_state)\n",
    "    test_size = int(len(df) * test_size)\n",
    "    df_train = df[:-test_size]\n",
    "    df_test = df[-test_size:]\n",
    "    return df_train, df_test\n",
    "\n",
    "def create_dataset(\n",
    "    df:pd.DataFrame, tokenmap:Dict[Dict[Union[str,int],Union(str,int)]]\n",
    "    )->Tuple[List[List[int]], List[int]]:\n",
    "    \"\"\"\n",
    "    Create dataset from dataframe.\n",
    "    \"\"\"\n",
    "    X = []\n",
    "    y = []\n",
    "    for _, row in df.iterrows():\n",
    "        text = row[\"text\"]\n",
    "        label = row[\"label\"]\n",
    "        text = preprocess_text(text, tokenmap)\n",
    "        X.append(text)\n",
    "        y.append(label)\n",
    "    return X, y\n",
    "\n",
    "def pad_sequences(\n",
    "    sequences:List[List[int]], maxlen:int, padding:str=\"post\", truncating:str=\"post\"\n",
    "    )->List[List[int]]:\n",
    "    \"\"\"\n",
    "    Pad sequences to the same length.\n",
    "    \"\"\"\n",
    "    padded = []\n",
    "    for sequence in sequences:\n",
    "        if len(sequence) > maxlen:\n",
    "            if truncating == \"post\":\n",
    "                sequence = sequence[:maxlen]\n",
    "            elif truncating == \"pre\":\n",
    "                sequence = sequence[-maxlen:]\n",
    "        elif len(sequence) < maxlen:\n",
    "            if padding == \"post\":\n",
    "                sequence = sequence + [0] * (maxlen - len(sequence))\n",
    "            elif padding == \"pre\":\n",
    "                sequence = [0] * (maxlen - len(sequence)) + sequence\n",
    "        padded.append(sequence)\n",
    "    return padded\n",
    "\n",
    "def create_embedding_matrix(\n",
    "    tokenmap:Dict[Dict[Union[str,int],Union(str,int)]], embedding_dim:int\n",
    "    )->np.ndarray:\n",
    "    \"\"\"\n",
    "    Create embedding matrix from glove embeddings.\n",
    "    \"\"\"\n",
    "    embedding_matrix = np.zeros((len(tokenmap[\"tok_to_idx\"]), embedding_dim))\n",
    "    with open(\"glove.6B.100d.txt\", \"r\") as f:\n",
    "        for line in f:\n",
    "            values = line.split()\n",
    "            word = values[0]\n",
    "            if word in tokenmap[\"tok_to_idx\"]:\n",
    "                idx = tokenmap[\"tok_to_idx\"][word]\n",
    "                embedding_matrix[idx] = np.asarray(values[1:], dtype=\"float32\")\n",
    "    return embedding_matrix\n",
    "\n",
    "def init_rnn(\n",
    "    embedding_dim:int, vocab_size:int, embedding_matrix:np.ndarray, \n",
    "    rnn_units:int, batch_size:int\n",
    "    )->Tuple[tf.keras.Model, tf.keras.Model]:\n",
    "    \"\"\"\n",
    "    Initialize RNN model.\n",
    "    \"\"\"\n",
    "    #encoder\n",
    "    encoder = tf.keras.Sequential([\n",
    "        tf.keras.layers.Embedding(\n",
    "            vocab_size, embedding_dim, \n",
    "            weights=[embedding_matrix], trainable=False\n",
    "        ),\n",
    "        tf.keras.layers.Bidirectional(\n",
    "            tf.keras.layers.LSTM(rnn_units, return_sequences=True)\n",
    "        ),\n",
    "        tf.keras.layers.Bidirectional(\n",
    "            tf.keras.layers.LSTM(rnn_units, return_sequences=True)\n",
    "        ),\n",
    "        tf.keras.layers.Bidirectional(\n",
    "            tf.keras.layers.LSTM(rnn_units)\n",
    "        )\n",
    "    ])\n",
    "    #decoder\n",
    "    decoder = tf.keras.Sequential([\n",
    "        tf.keras.layers.Dense(1, activation=\"sigmoid\")\n",
    "    ])\n",
    "    return encoder, decoder\n",
    "\n",
    "def loss_function(\n",
    "    y_true:tf.Tensor, y_pred:tf.Tensor\n",
    "    )->tf.Tensor:\n",
    "    \"\"\"\n",
    "    Custom loss function.\n",
    "    \"\"\"\n",
    "    loss = tf.keras.losses.BinaryCrossentropy(\n",
    "        from_logits=False, reduction=tf.keras.losses.Reduction.NONE\n",
    "    )\n",
    "    loss = loss(y_true, y_pred)\n",
    "    return loss\n",
    "\n",
    "def train_step(\n",
    "    encoder:tf.keras.Model, decoder:tf.keras.Model, \n",
    "    optimizer:tf.keras.optimizers.Optimizer, \n",
    "    x:tf.Tensor, y_true:tf.Tensor\n",
    "    )->Tuple[tf.Tensor, tf.Tensor]:\n",
    "    \"\"\"\n",
    "    Train step.\n",
    "    \"\"\"\n",
    "    with tf.GradientTape() as tape:\n",
    "        y_pred = decoder(encoder(x))\n",
    "        loss = loss_function(y_true, y_pred)\n",
    "    gradients = tape.gradient(loss, encoder.trainable_variables + decoder.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(gradients, encoder.trainable_variables + decoder.trainable_variables))\n",
    "    return loss, y_pred\n",
    "\n",
    "\n",
    "def main():\n",
    "    df = pd.read_csv(\"data.csv\")\n",
    "    df_train, df_test = test_train_split(df)\n",
    "    df_train.to_csv(\"train.csv\", index=False)\n",
    "    df_test.to_csv(\"test.csv\", index=False)\n",
    "    tokens = []\n",
    "    for _, row in df_train.iterrows():\n",
    "        text = row[\"text\"]\n",
    "        text = tokenize(text)\n",
    "        tokens.append(text)\n",
    "    tokenmap = create_token_maps(tokens)\n",
    "    with open(\"token map.json\", \"w\") as f:\n",
    "        json.dump(tokenmap, f)\n",
    "    X_train, y_train = create_dataset(df_train, tokenmap)\n",
    "    X_test, y_test = create_dataset(df_test, tokenmap)\n",
    "    X_train = pad_sequences(X_train, maxlen=100)\n",
    "    X_test = pad_sequences(X_test, maxlen=100)\n",
    "    embedding_matrix = create_embedding_matrix(tokenmap, embedding_dim=100)\n",
    "    np.save(\"embedding_matrix.npy\", embedding_matrix)\n",
    "    with open(\"train.json\", \"w\") as f:\n",
    "        json.dump({\"X\": X_train, \"y\": y_train}, f)\n",
    "    with open(\"test.json\", \"w\") as f:\n",
    "        json.dump({\"X\": X_test, \"y\": y_test}, f)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
